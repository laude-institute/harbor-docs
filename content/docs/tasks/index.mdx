---
title: Task Structure
description: Creating and running tasks for agentic environments
---

The Harbor task format is designed to be maximally flexible while still being intuitive to implement. The differences between the Harbor task format and the Terminal-Bench task format are [documented here](/docs/tasks/task-difference).

## Creating a task

To create a task, you can use the following command:

```bash
harbor tasks init "<task-name>"
```

This will create a new task directory with the following structure:

import { File, Folder, Files } from 'fumadocs-ui/components/files';

<Files>
  <File name="instruction.md" />
  <File name="task.toml" />
  <Folder name="environment" defaultOpen>
    <File name="Dockerfile" />
    <File name="..." />
  </Folder>
  <Folder name="solution" defaultOpen>
    <File name="solve.sh" />
    <File name="..." />
  </Folder>
  <Folder name="tests" defaultOpen>
    <File name="test.sh" />
    <File name="..." />
  </Folder>
</Files>

You can then populate the files with your task's content.

To evaluate an agent on your task, you can use the following command:

```bash
harbor run -p "<path/to/task>" -a "<agent>" -m "<model>"
```

## Task files

### Instruction

The instruction is a markdown file that contains the task's instruction.

### Configuration & Metadata

The `task.toml` file contains the task's configuration and metadata. Metadata is arbitrary and can consist of any information a task developer wants. Config params are nested into their respective sections rather than flat.

An example is shown below:

```toml
version = "1.0"

[metadata]
author_name = "Steve Jobs"
author_email = "steve@apple.com"
difficulty = "easy"
category = "programming"
tags = ["trivial"]

[verifier]
timeout_sec = 120.0
env = { API_KEY = "sk-test-123" }

[agent]
timeout_sec = 120.0

[solution]
env = { API_KEY = "sk-test-123" }

[environment]
build_timeout_sec = 600.0
docker_image = "some-org/some-name:some-tag"
cpus = 1
memory_mb = 2048
storage_mb = 10240
gpus = 0
gpu_types = ["H100", "A100"]
allow_internet = true

[[environment.mcp_servers]]
name = "mcp-server"
transport = "streamable-http"
url = "http://mcp-server:8000/mcp"
```

The configuration parameters are shown below:

import { TypeTable } from 'fumadocs-ui/components/type-table';

<TypeTable
  type={{
    "version": {
      description: "Version of the task configuration format.",
      type: "string",
      default: '"1.0"',
      path: "version",
      required: true,
    },
    "metadata": {
      description: "Arbitrary metadata provided by the task author.",
      type: "object",
      path: "metadata"
    },
    "verifier.timeout_sec": {
      description: "Timeout in seconds for the verifier.",
      type: "number",
      default: 600.0,
      path: "verifier.timeout_sec"
    },
    "verifier.env": {
      description: "Environment variables to set when running the verifier.",
      type: "object",
      default: "{}",
      path: "verifier.env"
    },
    "agent.timeout_sec": {
      description: "Timeout in seconds for the agent.",
      type: "number",
      default: 600.0,
      path: "agent.timeout_sec"
    },
    "solution.env": {
      description: "Environment variables to set when running the solution.",
      type: "object",
      default: "{}",
      path: "solution.env"
    },
    "environment.build_timeout_sec": {
      description: "Timeout in seconds for environment build.",
      type: "number",
      default: 600.0,
      path: "environment.build_timeout_sec"
    },
    "environment.docker_image": {
      description: "A pre-built Docker image to use for the environment.",
      type: 'string | null',
      default: null,
      path: "environment.docker_image"
    },
    "environment.cpus": {
      description: "Number of CPUs available to the environment.",
      type: "integer",
      default: 1,
      path: "environment.cpus"
    },
    "environment.memory_mb": {
      description: "Amount of RAM available to the environment in megabytes.",
      type: "integer",
      default: 2048,
      path: "environment.memory_mb"
    },
    "environment.storage_mb": {
      description: "Amount of storage available to the environment in megabytes.",
      type: "integer",
      default: 10240,
      path: "environment.storage_mb"
    },
    "environment.gpus": {
      description: "Number of GPUs available to the environment.",
      type: "integer",
      default: 0,
      path: "environment.gpus"
    },
    "environment.gpu_types": {
      description: "List of acceptable GPU types (e.g., ['H100', 'A100', 'T4']). None means any GPU type is acceptable.",
      type: 'list[string] | null',
      default: null,
      path: "environment.gpu_types"
    },
    "environment.allow_internet": {
      description: "Whether to allow internet access in the environment.",
      type: "boolean",
      default: true,
      path: "environment.allow_internet"
    },
    "environment.mcp_servers": {
      description: 'List of MCP servers to make available to the agent. Each entry has a name, transport ("sse", "streamable-http", or "stdio"), and transport-specific fields (url for sse/streamable-http, command/args for stdio). Compatible agents will register these servers automatically.',
      type: "list[MCPServerConfig]",
      default: "[]",
      path: "environment.mcp_servers"
    },
    "source": {
      description: "Optional source string for task provenance.",
      type: 'string | null',
      default: null,
      path: "source"
    }
  }}
/>

### Environment

The environment definition is placed in an `environment/` folder. **Harbor does not require any specific file to exist in that directory**. Which file is required depends on the environment type being used for the evaluation. For example, to use `--env docker`, the `DockerEnvironment` class checks that an `environment/Dockerfile` or `environment/docker-compose.yaml` is present. Different environment types could require other files to be present (e.g. an Apptainer environment could check for an `image.def` file). Most cloud sandbox providers only support `Dockerfile` defined environments and not docker compose.

There are a few special paths in the environment's filesystem:

| Path                   | Description                                                                                                       |
|------------------------|-------------------------------------------------------------------------------------------------------------------|
| `/logs/verifier/`      | Contains the reward file and other verifier logs.          |
| `/logs/agent/`         | A directory agents can use to store logs from their runs.                                                              |
| `/solution/`             | The solution folder is copied here by the Oracle agent at runtime and executed from the working directory.         |
| `/tests/`              | The tests folder is copied here by the Harbor harness at runtime and executed from the working directory.          |

The `/logs/` directory is downloaded to the host after the agent/verifier run and are often useful for debugging and analysis.

### Solution (Optional)

The solution folder must contain a `solution/solve.sh` script. Other dependencies are allowed. This folder is copied to `/solution` by the Oracle agent at runtime and executed from the working directory.

If no solution is provided, the Oracle agent cannot be used to sanity check the task.

### Tests

The tests folder must contain a `tests/test.sh` script. The test script should install test dependencies and verify the agent completed the instruction. In Terminal-Bench, this was done by running a `pytest` command, but this is now left to the task developer.

Other dependencies are allowed in the `tests/` folder. This folder is copied to `/tests` by the Harbor harness at runtime and executed from the working directory. E.g. `bash /tests/test.sh` is executed from `/app` in many cases.

**We recommend using absolute paths in your test script to avoid relative path issues.**

Importantly, the test script must produce a reward file in the `/logs/verifier/` directory. This is the file that the verifier will read to determine if the task was successful.

There are two ways to produce a reward file:


| Reward File           | Format                    | Description                                                                                   |
|-----------------------|---------------------------|-----------------------------------------------------------------------------------------------|
| `/logs/verifier/reward.txt`          | Plain text (e.g. `1`)     | A plain text file containing a single integer or float value, typically `1` for success or `0` for failure.  |
| `/logs/verifier/reward.json`         | JSON (e.g. `{ "runtime_sec": 1.23, "accuracy": 0.95, ... }`) | A JSON file that can define multiple metrics as rewards, but they must be floats or integers.           |

You may use either `reward.txt` or `reward.json` as the output of your test script. Harbor will read `reward.txt` by default and fall back to `reward.json`.

Often, a reward can be determined by the exit code or a unit test command.

```bash title="tests/test.sh"
#!/bin/bash

uvx pytest /tests/test.py

if [ $? -eq 0 ]; then
  echo 1 > /logs/verifier/reward.txt
else
  echo 0 > /logs/verifier/reward.txt
fi
```